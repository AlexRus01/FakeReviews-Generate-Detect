# -*- coding: utf-8 -*-
"""Train-RoGPT2.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Xrq3tYevteWOMTG7zNeOGz9xjFDF_Aoy
"""

!pip install transformers datasets -q
!pip install -U datasets
!pip install wandb

import os
os.environ["HF_DATASETS_OFFLINE"] = "1"
os.environ["WANDB_DISABLED"] = "true"

from google.colab import drive
drive.mount('/content/drive')

from datasets import load_dataset
from transformers import (
    AutoTokenizer,
    AutoModelForCausalLM,
    DataCollatorForLanguageModeling,
    Trainer,
    TrainingArguments,
)

dataset = load_dataset("text", data_files={"train": "romanian_real_reviews (1).txt"})

checkpoint = "readerbench/RoGPT2-medium"
tokenizer = AutoTokenizer.from_pretrained(checkpoint)
tokenizer.pad_token = tokenizer.eos_token
model = AutoModelForCausalLM.from_pretrained(checkpoint)

def tokenize(batch):
    return tokenizer(batch["text"], truncation=True, padding="max_length", max_length=128)

tokenized = dataset.map(tokenize, batched=True, remove_columns=["text"])
data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)

training_args = TrainingArguments(
    output_dir="/content/drive/MyDrive/roGPT2-finetuned",
    per_device_train_batch_size=2,
    num_train_epochs=2,
    save_steps=500,
    save_total_limit=1,
    logging_steps=100,
    fp16=True
)

trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=tokenized["train"],
    data_collator=data_collator,
)

trainer.train()

model.save_pretrained("/content/drive/MyDrive/roGPT2-finetuned")
tokenizer.save_pretrained("/content/drive/MyDrive/roGPT2-finetuned")